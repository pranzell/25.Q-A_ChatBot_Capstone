{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question - Answer Conversation Bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Total Documents = 442\n",
    "- Total inbuild questions = 87,432\n",
    "\n",
    "#### METHOD\n",
    "- Step I: Pass a query and it will point to a Document using normalized values of (X0.BM25 + X1.TFIDF + X2.DOC2VEC) linear equation \n",
    "- Step II: Which in turn will point to a paragraph (8lines) using WMD on the whole document (picked up by step I.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INPUT : Question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUTPUT: A paragraph within a document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --- Importing Various packages ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as scipy\n",
    "\n",
    "# Tokenizers\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "# Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# --- GENSIM PACKAGE ---\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, doc2vec, Doc2Vec\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.summarization.bm25 import BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_json('data/squad_train_doc.json')\n",
    "data_train.rename(columns={'passages': 'documents'}, inplace=True)\n",
    "\n",
    "# # Contains the list all titles\n",
    "title_list = np.load('title_list.npy').tolist()\n",
    "\n",
    "# Contains the dictionary of title to context\n",
    "dictionary_document_context = np.load('dictionary_document_context.npy').item()\n",
    "\n",
    "tokenized_context_and_questions = np.load('tokenized_context_and_questions.npy').tolist()\n",
    "\n",
    "untokenized_context_and_questions = np.load('untokenized_context_and_questions.npy').tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BM25 MODEL\n",
    "BM_25_model = BM25(tokenized_context_and_questions)\n",
    "\n",
    "# TFIDF MODEL\n",
    "dictionary = corpora.Dictionary.load('model_data/squad.dict')\n",
    "corpus = corpora.MmCorpus('model_data/squad.mm')\n",
    "TFIDF_model = gensim.models.TfidfModel.load('model_data/TFIDF_model.bin')\n",
    "\n",
    "# Doc2Vec Model\n",
    "Doc2Vec_model = gensim.models.Doc2Vec.load('model_data/Doc2Vec_model.bin')\n",
    "\n",
    "# WMD model \n",
    "WMD_model = KeyedVectors.load('model_data/WMD_model.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUERY Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BM25(query):    \n",
    "    ''' Accepts a question(query) to implement BM 25 Model.\n",
    "        Takes a query and word tokenizes it. \n",
    "              'get_scores' - Calculates the similarity distance between the tokenized-query and the document.\n",
    "\n",
    "        --> Returns a dataframe with Document name, Score and Rank\n",
    "    '''\n",
    "    scores = BM_25_model.get_scores(query.split(),1)\n",
    "    BM25_dataframe = pd.DataFrame({'Document':data_train.title, 'Score_BM25':scores}).sort_values(by=['Score_BM25'],ascending=False)\n",
    "    BM25_dataframe['Rank_BM25'] = [i for i in range(1, len(data_train.title)+1)]\n",
    "    return BM25_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TFIDF(query): \n",
    "    ''' Accepts a question(query) to implement TF-IDF Model.\n",
    "        Takes a query and word tokenizes it. \n",
    "        'raw_corpus_query' - The word-tokenized query is compared with the dictionary used to train the document. \n",
    "            'corpus_query' - The word-id and word is converted into a corpus.The corpus is then fed to the TF-IDF model.\n",
    "        'similarity_table' - Stores the TF-IDF weights which are then used to get most similiar documents.\n",
    "                   'ranks' - Scipy method which compares the similarity weights and sorts is accordingly.\n",
    "\n",
    "        --> Returns a dataframe with Document name, Score and Rank\n",
    "    '''\n",
    "    query_1 = []\n",
    "    query_1.append(word_tokenize(query))\n",
    "    raw_corpus_query = [dictionary.doc2bow(word) for word in query_1]\n",
    "    corpora.MmCorpus.serialize('model_data/query3.mm',raw_corpus_query)\n",
    "    corpus_query = corpora.MmCorpus('model_data/query3.mm')\n",
    "    \n",
    "    similarity_table = TFIDF_model[corpus_query]\n",
    "    ranks = scipy.rankdata(similarity_table, method = 'max')\n",
    "    similarity_table = list(np.array(similarity_table).flatten())\n",
    "    TFIDF_dataframe = pd.DataFrame({'Document':data_train.title, 'Score_TFIDF':similarity_table}).sort_values(by=['Score_TFIDF'],ascending=False)\n",
    "    TFIDF_dataframe['Rank_TFIDF'] = [i for i in range(1, len(data_train.title)+1)]\n",
    "    return TFIDF_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Doc2Vec(query):\n",
    "    ''' Accepts a question(query) to implement Doc2Vec Model.\n",
    "        Takes a query and word tokenizes it. \n",
    "           'avg_sentence' - After that the average of the sentenced words are compared with every document.\n",
    "           'most_similar' - Calculates the similarity distance between the avg of tokenized-sentence with every \n",
    "                            document iteratively.\n",
    "        'list_doc_scores' - Returns the sorted list of comparison with each doc in ascending order.\n",
    "\n",
    "        --> Returns a dataframe with Document name, Score and Rank(top_n, ascending order sorted)\n",
    "    '''\n",
    "\n",
    "    similarity_score_matrix , list_doc_names, list_doc_scores, list_doc_ranks, rank = [], [], [], [], 1\n",
    "    avg_sentence = np.zeros((200))\n",
    "    count = 0\n",
    "    for word in word_tokenize(query):\n",
    "        if word in Doc2Vec_model.wv.vocab:\n",
    "            avg_sentence +=  Doc2Vec_model[word]\n",
    "            count+=1\n",
    "    if count != 0:\n",
    "        avg_sentence = avg_sentence / count\n",
    "    similarity_score_matrix.append(Doc2Vec_model.docvecs.most_similar([avg_sentence], topn=len(title_list)))\n",
    "    for each_compared_row in similarity_score_matrix[0]:\n",
    "        list_doc_names.append(each_compared_row[0])\n",
    "        list_doc_scores.append(each_compared_row[1])\n",
    "        list_doc_ranks.append(rank)\n",
    "        rank += 1\n",
    "    query_comparison_dataframe = pd.DataFrame({'Document':list_doc_names, 'Score_Doc2Vec':list_doc_scores, 'Rank_Doc2Vec':list_doc_ranks})\n",
    "    return query_comparison_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query to Document Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query_to_document(query):\n",
    "    \"\"\" Takes string question and returns the name of the document which the question is likely to be present in\"\"\"\n",
    "    \n",
    "    bm25_df = BM25(query).head(n=20)         # gets the dataframe of BM25 with scores and ranks of documents\n",
    "    tfidf_df = TFIDF(query).head(n=20)       # gets the dataframe of TFIDF with scores and ranks of documents\n",
    "    doc2vec_df = Doc2Vec(query).head(n=20)   # gets the dataframe of Doc2Vec with scores and ranks of documents\n",
    "    \n",
    "    # combining all the dataframes\n",
    "    final_df = pd.merge(pd.merge(bm25_df,tfidf_df, on=['Document'], how='outer'), doc2vec_df, on=['Document'], how='outer')\n",
    "    final_df.fillna(0, inplace=True)\n",
    "    \n",
    "    # Normalising the scores between 0 and 1\n",
    "    bm25_normalised = (final_df.Score_BM25 - final_df.Score_BM25.min())/(final_df.Score_BM25.max()- final_df.Score_BM25.min())\n",
    "    tfidf_normalised = (final_df.Score_TFIDF-final_df.Score_TFIDF.min())/(final_df.Score_TFIDF.max()-final_df.Score_TFIDF.min())\n",
    "    doc2vec_normalised = (final_df.Score_Doc2Vec-final_df.Score_Doc2Vec.min())/(final_df.Score_Doc2Vec.max()-final_df.Score_Doc2Vec.min())\n",
    "    \n",
    "    # Getting the total score based on the preious overall accuracy\n",
    "    final_df['total_score'] = 0.01243557 * bm25_normalised + 0.29682442 * tfidf_normalised - 0.01673123 * doc2vec_normalised\n",
    "    \n",
    "    final_document = final_df.sort_values(by='total_score', ascending=False).Document[0]\n",
    "\n",
    "    return final_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document to Paragraph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def document_to_paragraph(query, document):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    sent1 = [word for word in word_tokenize(query) if word not in stop_words]\n",
    "    tag = nltk.pos_tag(sent1)\n",
    "    words = []\n",
    "    for each_tag in tag:\n",
    "        if each_tag[1] == 'NN' or each_tag[1] == 'NNP' or each_tag[1] == 'NNS' or each_tag[1] == 'VBD' or each_tag[1] == 'VB':\n",
    "            words.append(each_tag[0])\n",
    "    sent1 = words\n",
    "    index = 0\n",
    "    sentences = sent_tokenize(document)\n",
    "    list_distances, list_sentence_index = [], []\n",
    "    for each_sentence in sentences:\n",
    "        sent2 = [word for word in word_tokenize(each_sentence) if word not in stop_words]\n",
    "        similarity_distance = WMD_model.wmdistance(sent1, sent2)\n",
    "        list_distances.append(similarity_distance)\n",
    "        list_sentence_index.append(index)\n",
    "        index+=1\n",
    "    WMD_Dataframe = pd.DataFrame({'Sentence': sentences, 'Sentence_Index': list_sentence_index, 'WMD_Score': list_distances}).sort_values(by=['WMD_Score'],ascending=True) \n",
    "    Top8_sentences = ' '.join([sent for sent in WMD_Dataframe[0:8].Sentence])\n",
    "   \n",
    "    return Top8_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query_to_paragraph(query):\n",
    "    document_name = query_to_document(query)\n",
    "    document_context = dictionary_document_context[document_name]\n",
    "    paragraph = document_to_paragraph(query=query, document=document_context)\n",
    "    return paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query: Who is Beyonce?\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Enter your query: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'American_Idol'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_to_document(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paragraph = query_to_paragraph(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mariah Carey and Nicki Minaj left the panel after one season. 1 on the Billboard 200. They were replaced by three new judges, Mariah Carey, Nicki Minaj and Keith Urban, who joined Randy Jackson in season 12. This season's judging panel consisted of Randy Jackson, along with Mariah Carey, Keith Urban and Nicki Minaj. Initially the contestants sing one song each week, but this is increased to two songs from top four or five onwards, then three songs for the top two or three. Both judges Mariah Carey and Nicki Minaj also decided to leave after one season to focus on their music careers. The judging panel for the most recent season consisted of country singer Keith Urban, singer and actress Jennifer Lopez, and jazz singer Harry Connick, Jr. For the first time in the show's history, the top 5 contestants were all female.\n"
     ]
    }
   ],
   "source": [
    "print(paragraph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
